# Configuration File for Evaluation Script, look up ws, ss, k, d in corresponding experiment log file for experiment validation reconstruction

#################### GENERAL ###########################
# DATASET - either Cnn/Dailymail or Wikihow
dataset: "Wiki" # "CnnDm" or "Wiki" or "Pubmed"
# DUMP - WHETHER TO DUMP DATASET
dump: True
# DEVICE
device: "cpu" # "cuda:0" / "cuda:0" / "cuda:1" etc.
# NUMBER OF THREADS
num_threads: 1
# PIN MEMORY
pin_memory: False
# BATCH SIZE
batch_size: 2  # will be multiplied by 100 to get to the token-level-batch size
# NUMBER WORKERS FOR DATALOADER
loader_num_workers: 1
# SHUFFLE DATASET
shuffle: True
# TOY RUN
toy_run: True


#################### TRAINING ###########################
# LOSS CRITERION
criterion: "torch.nn.NLLLoss()"
# SAVING INTERVAL
save_interval: 3
# LOG INTERVAL - SHOW TOY OUTPUT ON TRAIN INSTANCE
log_interval: 2
# VALIDATION INTERVAL
val_interval:
# NUMBER OF EPOCHS
num_epochs: 21
# WEIGHT DECAY
weight_decay: 0.00001
# Reinforcement Learning
rl: False
# BREAKPOINT AFTER * INSTANCES
bp:
# LAMBDA FOR WEIGHTING MLE AND RL LOSS COMPONENTS
lambda_: 0.999  # 0.9984 (paper)

# WHETHER TO RESUME TRAINING FROM CHECKPOINT
resume: False
# WHICH EPOCH TO RESUME FROM
from_epoch: 20
# WHICH SAVE DESCRIPTION TO RESUME FROM
from_save_description: "XXXIII"

# SAVE DIRECTORY TO DUMP MODELS ACCORDING TO SAVING INTERVAL
save_dir: "data/models"
# SAVE DESCRIPTION
save_description: "XXXXX"

# OPTIMIZER (ADAM) SETTINGS
factor: 1
warmup: 4000
beta_1: 0.9
beta_2: 0.999 # 0.999 (default), 0.98 (Transformer)
opt_eps: 1e-8 # 1e-8 (default), 1e-9 (Transformer)


#################### EMBEDDING ###########################
# INITIALIZE EMBEDDINGS WITH PRETRAINED EMBEDDINGS
init: True
# PATH TO EMBEDDING FILE
emb_name: "data/embeddings/wiki.en.vec"
# EMBEDDING DIMENSIONALITY
emb_dim: 300
# MAXIMUM VOCABULARY SIZE
max_vocab: 50000
# NORMALIZE EMBEDDINGS
norm: True
# FINE TUNE EMBEDDINGS
trainable: True

#################### SEQ2SEQ #############################
encoder: "Recurrent" # Recurrent / Transformer
decoder: "Recurrent" # Recurrent / Transformer

# MAXIMUM INPUT SEQUENCE LENGTH FED TO ENCODER
enc_max_len: 740
# MAXIMUM OUTPUT SEQUENCE LENGTH GENERATED BY DECODER
dec_max_len: 125
# WHETHER TO TIE THE EMBEDDINGS IN INPUT AND OUTPUT LAYERS
tied_weights: True
# WHETHER TO EVALUATE WITH BEAM SEARCH
eval_beam: True
# SIZE OF BEAMS USED IN BEAMSEARCHDECODER
B: 3

#################### RECURRENT ###########################

# NUMBER OF LAYERS IN (BI-) LSTM/GRU -> stacked LSTM/GRU
num_layers: 1
# HIDDEN DIMENSIONALITY
hidden_dim: 256
# TYPE OF RNN
rnn_type: "GRU" # "LSTM" or "GRU"
# TYPE OF ATTENTION
attention_type: "Luong" # "Luong" or "Bahdanau"
# PROBABILITY FOR TEACHER FORCING DURING TRAINING
tf_prob: 1
# WHETHER TO USE BIASES IN VARIOUS INTERMEDIATE LAYERS
bias: True
# EPSILON PARAMETER FOR NUMERICAL STABILIZATION IN LOG-CALCULATION
eps: 1e-9
# WHETHER TO ENABLE POINTING / POINTER NETWORK
pointer: True

#################### TRANSFORMER ###########################
# NUMBER OF STACKS
N: 3
# NUMBER OF HEADS IN MULTI-HEAD-ATTENTION
h: 4
# DROPOUT
dropout: 0.1
# MODEL DIMENSION
d_model: 256
# POSTION-WISE FEEDFORWARD DIMENSION
d_ff: 1024

#################### WINDOWING ###########################
# WHETHER TO USE WINDOWING APPROACH
windowing: True
# WINDOWING TYPE - STATIC OR DYNAMIC
w_type: "dynamic"
# FIRST SKEWNESS PARAMETER IN WINDOWING SCHEDULER
k: 1.2
# SECOND SKEWNESS PARAMETER IN WINDOWING SCHEDULER
d: 0
# MAX CORPUS LEN
max_corpus_len: 1500 # CnnDm: 1500, Wiki: 800, Pubmed: 4000
# WINDOW SIZE
ws: 700
# STEP SIZE
ss: 680

